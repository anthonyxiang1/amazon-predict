# -*- coding: utf-8 -*-
"""hw3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15xU1FdZO-epSlRqnJavYnUW9TDxVjBvu
"""

import numpy as np
from numpy import genfromtxt
import pandas as pd

from sklearn.linear_model import Ridge #just like L2 regularized logistic regression
from sklearn.decomposition import PCA
from sklearn.preprocessing import normalize
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
import scipy.stats as ss #for distributions

from gensim.models import Word2Vec
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
from happiestfuntokenizing.happiestfuntokenizing import Tokenizer

# 1.1 Read the reviews and ratings from the file

import sys

trainFile = sys.argv[1][1:-1]
testFile = sys.argv[2][1:-1]


#trainFile = "food_train.csv"
#testFile = "food_trial.csv"
# trainFile = "music_train.csv"
# testFile = "music_trial.csv"
#trainFile = "musicAndPetsup_train.csv"
# testFile = "musicAndPetsup_trial.csv"
#testFile = "musicAndPetsup_test_noLabels.csv"

train = pd.read_csv(trainFile).to_numpy()
test = pd.read_csv(testFile).to_numpy()

# 1.2 Tokenize the file

tokenizer = Tokenizer()
train_tokenized = [tokenizer.tokenize(train[i][5]) for i in range(len(train))]
test_tokenized = [tokenizer.tokenize(test[i][5]) for i in range(len(test))]

# replace OOV
allWords = [j for i in train_tokenized for j in i]

counts = dict()
for i in allWords:
  counts[i] = counts.get(i, 0) + 1

oovCount = 0
oovDict = {}
vocabDict = {}
for k, v in counts.items():
    if (v < 3):
        oovCount += v
        oovDict.update({k:v})
    else:
        vocabDict.update({k:v})

vocabDict['<OOV>'] = oovCount
        
for i, review in enumerate(train_tokenized):
    for j in range(len(review)):
        if (train_tokenized[i][j] in oovDict):   # must be either oovDict or vocabDict
            train_tokenized[i][j] = '<OOV>'
            
for i, review in enumerate(test_tokenized):
    for j in range(len(review)):
        if (test_tokenized[i][j] not in vocabDict):   # count be in neither
            test_tokenized[i][j] = '<OOV>'

# 1.3 Use GenSim word2vec to train a 128-dimensional word2vec model utilizing only the training data

w2vmodel = Word2Vec(train_tokenized, min_count=1,size= 128,workers=3, window =3, sg = 1)

# 1.4 Extract features

def getRepresentation(reviewTokenized):
    '''
    input: list of words (for each review)
    output: list of 128 numbers (average of embeddings)
    '''
    word2vec = w2vmodel[reviewTokenized]
    a = np.array(word2vec).astype(np.float)
    
    return np.mean(a, axis=0)

trainEmbedding = [getRepresentation(row) for row in train_tokenized]  
testEmbedding = [getRepresentation(row) for row in test_tokenized]

# dev set for finding best alpha

X_train, X_dev, y_train, y_dev = train_test_split(trainEmbedding, train[:,1], test_size=0.2, random_state=42)

a = [3e-7, 3e-6, .00003, .0003, .003, .03, .3, 3, 30, 300]
best_r = 0
best_a = .003

for i in a:
    clf = Ridge(alpha=i)
    clf.fit(X_train, y_train)
    y_pred = np.clip(clf.predict(X_dev), 1, 5)
    result = ss.pearsonr(y_dev, y_pred)[0]
    if (result > best_r):
        best_r = result
        best_a = i

# 1.5 Build a rating predictor using L2 *linear* regression

y = train[:,1]
clf = Ridge(alpha=best_a)
clf.fit(trainEmbedding, y)
w = clf.coef_

print("train")
print("MAE: ", mean_absolute_error(train[:,1], clf.predict(trainEmbedding)))
print("Pearson R: ", ss.pearsonr(train[:,1], clf.predict(trainEmbedding))[0])

# 1.6 Run the predictor on the second set of data and print both the mean absolute error and then Pearson correlation

y_pred = np.clip(clf.predict(testEmbedding), 1, 5)  # clip predictions to between 1 and 5

print("trial")
print("MAE: ", mean_absolute_error(test[:,1], y_pred))
print("Pearson R: ", ss.pearsonr(test[:,1], y_pred)[0])

# making the csv for stage 1 competition

# ids = np.array(test[:,0])
# result = np.array(y_pred)
# dataset = pd.DataFrame({'id': ids, 'Predicted': result }, columns=['id', 'Predicted'])
# dataset.to_csv("prediction.csv", index=False)

# Stage 1 Checkpoint

foodCheckpoint = [548, 4258, 4766, 5800]
musicCheckpoint = [329, 11419, 14023, 14912]

print("y", "", "y_pred")
if (trainFile == "food_train.csv"):
    for i in foodCheckpoint:
        print(test[np.where(test[:,0] == i)][0][1], "", y_pred[np.where(test[:,0] == i)[0][0]])

elif (trainFile == "music_train.csv"):
    for i in musicCheckpoint:
        print(test[np.where(test[:,0] == i)][0][1], "", y_pred[np.where(test[:,0] == i)[0][0]])

### STAGE 2

# 2.1 Grab the user_ids for both datasets

train_id = train[:,4]
test_id = test[:,4]

# 2.2 For each user, treat their training data as "background"

userTrainDict = {}
for i, review in enumerate(train_tokenized):
    if (train_id[i] in userTrainDict):
        userTrainDict[train_id[i]].append(getRepresentation(review))
    else:
        userTrainDict[train_id[i]] = [getRepresentation(review)]
        
userTestDict = {}
for i, review in enumerate(test_tokenized):
    if (test_id[i] in userTestDict):
        userTestDict[test_id[i]].append(getRepresentation(review))
    else:
        userTestDict[test_id[i]] = [getRepresentation(review)]

for k,v in userTrainDict.items():
    if len(v) > 1:
        userTrainDict[k] = np.mean(np.array(v).astype(np.float), axis=0)
    else:
        userTrainDict[k] = userTrainDict[k][0]
    
userTrainKeys = list(userTrainDict.keys())
userTrainValues = list(userTrainDict.values())

for k,v in userTestDict.items():
    if len(v) > 1:
        userTestDict[k] = np.mean(np.array(v).astype(np.float), axis=0)
    else:
        userTestDict[k] = userTestDict[k][0]
    
userTestKeys = list(userTestDict.keys())
userTestValues = list(userTestDict.values())

# now, we have a N users by 128 embeddings dictionary

# normalize data first

userTrainValuesNormed = normalize(userTrainValues, axis=0)
userTestValuesNormed = normalize(userTestValues, axis=0)

# 2.3 Run PCA the matrix of user-language representations to reduce down to just three factors

V = PCA(n_components=3)
trainComponents = V.fit_transform(userTrainValuesNormed)   # N users x 3
testComponents = V.transform(userTestValuesNormed)

# 2.4 Use the first three factors from PCA as user factors
# in order to run user-factor adaptation, otherwise using the same approach as stage 1.

trainfactorDict = dict(zip(userTrainKeys, trainComponents)) 
testfactorDict = dict(zip(userTestKeys, testComponents)) 

trainEmbeddingWithUser = [np.array([getRepresentation(train_tokenized[i])*factor for factor in trainfactorDict[train_id[i]]]).flatten() for i,row in enumerate(train_tokenized)]
testEmbeddingWithUser = [np.array([getRepresentation(test_tokenized[i])*factor for factor in testfactorDict[test_id[i]]]).flatten() for i,row in enumerate(test_tokenized)]

# dev set for finding best alpha

X_train2, X_dev2, y_train2, y_dev2 = train_test_split(trainEmbeddingWithUser, train[:,1], test_size=0.2, random_state=42)

a2 = [3e-7, 3e-6, .00003, .0003, .003, .03, .3, 3, 30, 300]
best_r2 = 0
best_a2 = .003

for i in a2:
    clf2 = Ridge(alpha=i)
    clf2.fit(X_train2, y_train2)
    y_pred2 = np.clip(clf2.predict(X_dev2), 1, 5)
    result2 = ss.pearsonr(y_dev2, y_pred2)[0]
    if (result2 > best_r2):
        best_r2 = result2
        best_a2 = i

y2 = train[:,1]
clf2 = Ridge(alpha=best_a2)
clf2.fit(trainEmbeddingWithUser, y2)
w2 = clf2.coef_

print("train")
print("MAE: ", mean_absolute_error(train[:,1], clf2.predict(trainEmbeddingWithUser)))
print("Pearson R: ", ss.pearsonr(train[:,1], clf2.predict(trainEmbeddingWithUser))[0])

y_pred2 = np.clip(clf2.predict(testEmbeddingWithUser), 1, 5)  # clip predictions to between 1 and 5
print("trial")
print("MAE: ", mean_absolute_error(test[:,1], y_pred2))
print("Pearson R: ", ss.pearsonr(test[:,1], y_pred2)[0])

# normalizing gave a better predicted pearson R
# not normalizing gave better training fit (possibly overfit)
# either way, lower than our model without user adaption

# submission for stage 2 results
# ids = np.array(test[:,0])
# result = np.array(y_pred2)
# dataset = pd.DataFrame({'id': ids, 'Predicted': result }, columns=['id', 'Predicted'])
# dataset.to_csv("predictionStage2.csv", index=False)

# Stage 2 Checkpoint

print("y", "", "y_pred")
if (trainFile == "food_train.csv"):
    for i in foodCheckpoint:
        print(test[np.where(test[:,0] == i)][0][1], "", y_pred2[np.where(test[:,0] == i)[0][0]])

elif (trainFile == "music_train.csv"):
    for i in musicCheckpoint:
        print(test[np.where(test[:,0] == i)][0][1], "", y_pred2[np.where(test[:,0] == i)[0][0]])

# 3.1  Start with word2vec embeddings *per word*

trainEmbeddingWords = [w2vmodel[row] for row in train_tokenized]
testEmbeddingWords = [w2vmodel[row] for row in test_tokenized]

# list of 2d np arrays

# get max length to pad for

max = 0
count = 0
for i in train_tokenized:
    count += len(i)
    if len(i) > max:
        max = len(i)

for i in test_tokenized:
    count += len(i)
    if len(i) > max:
        max = len(i)

avgCount = count / (len(test_tokenized + train_tokenized))

trainEmbeddingWordsPad = []  # pad them to the longest review length / 2 - to reduce the training time, given that not much new information can be determined from the later half of a review
pad_len = int(avgCount)
# 3.4 Add the user factors from stage 2 to your deep learning module - Add the factor values as additional tokens to the end

for i,embed in enumerate(trainEmbeddingWords):
    if (len(embed) >= pad_len):
        padded = embed[0:pad_len]
        factorInput = np.array(np.pad(trainfactorDict[train_id[i]], (0,125), 'constant'))
        new = np.insert(padded, len(padded), factorInput, axis=0)
        trainEmbeddingWordsPad.append(new)
    else:
        cat = np.zeros((pad_len - len(embed), 128))
        factorInput = np.array(np.pad(trainfactorDict[train_id[i]], (0,125), 'constant'))
        padded = np.concatenate((embed,cat), axis=0)
        new = np.insert(padded, len(padded), factorInput, axis=0)
        trainEmbeddingWordsPad.append(new)
    
# trainEmbeddingWordsPad is a list of 2d arrays (max+1, 128) - the embedding, then 0's for padding, then last row is factors

testEmbeddingWordsPad = []

for i,embed in enumerate(testEmbeddingWords):
    if (len(embed) >= pad_len):
        padded = embed[0:pad_len]
        factorInput = np.array(np.pad(testfactorDict[test_id[i]], (0,125), 'constant'))
        new = np.insert(padded, len(padded), factorInput, axis=0)
        testEmbeddingWordsPad.append(new)
    else:
        cat = np.zeros((pad_len - len(embed), 128))
        factorInput = np.array(np.pad(testfactorDict[test_id[i]], (0,125), 'constant'))
        padded = np.concatenate((embed,cat), axis=0)
        new = np.insert(padded, len(padded), factorInput, axis=0)
        testEmbeddingWordsPad.append(new)

# 3.2 Input the word2vec embeddings as the input sequence

# LSTM MODEL, skeleton of the code comes from Matthew Matero's Pytorch Intro tutorial, which he presented in class

if torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")
    
class LSTMRegression(nn.Module):
    def __init__(self, input_dim, hidden_dim, drop_prob):
        super(LSTMRegression, self).__init__()
        
        self.h = hidden_dim
        self.input = input_dim
        
        self.lstm = nn.LSTMCell(input_dim, hidden_dim)
        self.do = nn.Dropout(p=drop_prob)
        self.reg = nn.Linear(hidden_dim, 1)
        
    def forward(self, inputs, hidden):
        seq_len = inputs.shape[0]
        
        hidden_state, cell_state = hidden
        
        hidden_states = []
        for i in range(seq_len):
            hidden_state, cell_state = self.lstm(inputs[i], (hidden_state, cell_state))
            hidden_states.append(hidden_state)
            
        return self.reg(self.do(hidden_states[-1])) # return the regressed last hidden state
    
    def init_hidden(self, batch_size):
        w = next(self.parameters())
        
        hidden, cell = (w.new_zeros(batch_size, self.h), w.new_zeros(batch_size, self.h))
        
        return hidden, cell

def train(trainData, epochs):
    
    model.to(device)
    model.train()
    
    for i in range(1, epochs+1):
        epoch_loss = []

        for x, label in trainData:
            
            optimizer.zero_grad()
            hidden = model.init_hidden(x.shape[0])
            
            data = x.transpose(0,1).float().to(device)
            # data shape - max+1, batchsize, embeddingsize

            preds = model(data, hidden)

            loss = criterion(preds, label.to(device))

            loss.backward()

            optimizer.step()

            epoch_loss.append(loss.item())
            
        if (i % 10 == 0):
            print(f'Loss for epoch #{i}: {np.mean(epoch_loss)}')

def eval(testdata):
    model.to(device)
    model.eval()

    with torch.no_grad():
        test_loss = []
        predsList = []

        for x, label in testdata:
            hidden = model.init_hidden(x.shape[0])
            data = x.transpose(0,1).float().to(device)

            preds = torch.clamp(model(data, hidden), 1,5)
            #print(preds)
            predsList.append(preds.squeeze(1).tolist())

            loss = criterion(preds, label.to(device))
            test_loss.append(loss.item())

    print(f'Loss for test: {np.mean(test_loss)}')

    resultant = [item for sublist in predsList for item in sublist]
    return resultant

# hyperparameters
batch_size = 400
inputDim = 128
hiddenDim = 64
drop_prob = 0.2
lr = 1e-3
epochs = 50

# load in data

trainDataset = TensorDataset(torch.from_numpy(np.array(trainEmbeddingWordsPad)), 
                              torch.from_numpy(np.array(y, dtype=np.float32))[:, None] )

#trainDataset = TensorDataset(torch.randn((100, 3, 128)), torch.randn((100,1))

trainData = DataLoader(trainDataset, batch_size=batch_size, num_workers=0)

#testDataset = TensorDataset(torch.randn((100, 3, 128)), torch.randn((100,1)))
testDataset = TensorDataset(torch.from_numpy(np.array(testEmbeddingWordsPad)), 
                           torch.from_numpy(np.array(test[:,1], dtype=np.float32))[:, None]   )
testData = DataLoader(testDataset, batch_size=batch_size, num_workers=0)

# make model
model = LSTMRegression(inputDim, hiddenDim, drop_prob)

optimizer = torch.optim.Adam(model.parameters(), lr = lr)
criterion = nn.MSELoss()

# train and evaluate
train(trainData, epochs)

resultantTrain = eval(trainData)
resultantTest = eval(testData)

compare = pd.DataFrame(zip(resultantTest, test[:,1]), columns=['pred','label'])
print(compare)

print("Pearson R: ", ss.pearsonr(y, resultantTrain)[0])
print("Pearson R: ", ss.pearsonr(test[:,1], resultantTest)[0])

# for kaggle
#whatwewant = pd.DataFrame(zip(test[:,0], resultantTest), columns=['id','Predicted'])
#whatwewant.to_csv(r'predictionFinal.csv', index = False, header=True)
#!cp predictionFinal.csv "drive/My Drive/Colab Notebooks"